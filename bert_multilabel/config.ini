[INPUT]
BERT_VOCAB = ./uncased_L-12_H-768_A-12/vocab.txt
BERT_INIT_CHKPNT = ./uncased_L-12_H-768_A-12/bert_model.ckpt
BERT_CONFIG = ./uncased_L-12_H-768_A-12/bert_config.json
CORPUS_DIR =

[TRAINING_PARAMETERS]
# We'll set sequences to be at most 128 tokens long.
MAX_SEQ_LENGTH = 128
# Compute train and warmup steps from batch size
# These hyperparameters are copied from this colab notebook
BATCH_SIZE = 32
LEARNING_RATE = 2e-5
NUM_TRAIN_EPOCHS = 10
# Warmup is a period of time where hte learning rate
# is small and gradually increases--usually helps training.
WARMUP_PROPORTION = 0.1
# Model configs
SAVE_CHECKPOINTS_STEPS = 1000
SAVE_SUMMARY_STEPS = 500

[OUTPUT]
OUTPUT_DIR=./working/output
RESULTS_FILE = multilabel_results.csv

